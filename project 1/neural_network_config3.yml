learning_rate: 0.001

#how many sample consider for each iteration
batch_size: 16

#how many run for the training set to completing the training
epochs: 250

#for displaying or not informatino about the run
verbose: True

# units = number of neurons. must be in range [1,1000]
# activation functions [none, sigmoid, tanh, relu, linear]
layers:
  - units: 400
    activation: none
  - units: 200
    activation: sigmoid
  - units: 150
    activation: sigmoid
  - units: 100
    activation: sigmoid
  - units: 50
    activation: sigmoid
  - units: 50
    activation: sigmoid
  - units: 4
    activation: linear

# [True, False] to have or not a softmax function for last layer 
softmax: True

# [MSE, cross_entropy]
loss_function: MSE

# [L1, L2, none]
regularaizer: none

# Typically a small fraction
regularaizer_factor: 0.001

# Initial weight ranges for each (non-input) layer,
# options: [glorot, [low, high]]
# where [low, high] result in low <= weight <= high ranges.
initial_weight_ranges: glorot

# Initial bias ranges on the form [low, high] such that low <= bias <= high
initial_bias_ranges: [0, 1]

